# ОТВЕТЫ НА КРИТИЧЕСКИЕ ВОПРОСЫ

**Дата:** 2025-11-01  
**Статус:** Черновик для согласования с Ольгой

---

## ПРЕДВАРИТЕЛЬНЫЙ АНАЛИЗ

Предлагаю следующие ответы на критические вопросы, основанные на контексте проекта и принципе "Усилия → 0".

---

## Вопрос 1: Доступ и Легальность (Приватные Источники)

### Предлагаемое решение:

**Фаза 1 (MVP):** Только публичные источники
- Веб (поиск по "Ольга Розет" + контекст)
- LinkedIn (публичный профиль)
- СМИ, публикации, упоминания
- **Обоснование:** Нулевой юридический риск, быстрый старт

**Фаза 2 (Опциональная):** Приватные источники с explicit opt-in
- Интеграция с почтой (Gmail API) только после получения OAuth consent
- Обработка только деловой переписки (исключить личную через filtering)
- **Compliance:** Полный GDPR compliance (data provenance, right to be forgotten)

**Рекомендация:** Начать с Фазы 1. Оценить качество графа. Принять решение о Фазе 2 на основе ROI.

---

## Вопрос 2: Приоритет Ошибок (Precision vs. Recall)

### Предлагаемое решение:

**Приоритет: Precision > Recall**

**Обоснование:**
- False Positive (неверная связь) — катастрофическая ошибка. Может повредить репутации или привести к неуместным контактам.
- False Negative (пропущенная связь) — приемлемо. Система со временем восполнит пробелы через continuous ingestion.

**Параметры:**
- Confidence threshold для E: **> 0.95** (очень консервативно)
- Entity Resolution threshold: **> 0.99** (слияние только при практически 100% уверенности)
- Human verification: Опциональный режим "review queue" для E с 0.85 < confidence < 0.95

---

## Вопрос 3: Проблема "Холодного Старта" (Bootstrap)

### Предлагаемое решение:

**Hybrid approach:**

1. **Zero-shot baseline:** Начать с Open Relation Extraction (OIE)
   - LLM извлекает любые тройки (Субъект, Предикат, Объект)
   - Система не пытается предсказать фиксированный набор отношений

2. **Domain-specific prompting:**
   - Предоставить LLM контекст домена Ольги Розет:
     ```
     Контекст: Ольга — дизайнер, преподаватель, куратор выставок.
     Релевантные типы связей: куратор_выставки, со-куратор, 
     преподаватель_в, участник_резиденции, клиент, партнёр, коллега.
     ```
   - Это few-shot без необходимости training data

3. **Self-learning ontology:**
   - Система анализирует частотность извлечённых предикатов
   - Автоматически формирует онтологию (e.g., "со-куратор" встречается часто → добавить в таксономию)

**Отказ от synthetic data:** Не использовать LLM-generated synthetic data. Приоритет — высокая точность реальных данных.

---

## Вопрос 4: Инфраструктура и Затраты

### Предлагаемое решение:

**Гибридный подход (оптимальное соотношение cost/complexity):**

**Для MVP:**
- **LLM:** API-based (GPT-4o / Claude 3.5 Sonnet)
  - Причина: Максимальное качество NER/RE из коробки
  - TCO: ~$50-200/месяц (зависит от объёма)
  
- **Инфраструктура:** Managed services (cloud)
  - Graph DB: Neo4j AuraDB (managed) или AWS Neptune
  - Ingestion: AWS Lambda + SQS (event-driven, pay-per-use)
  - Storage: S3 (data lake)
  
- **Orchestration:** Serverless (AWS Step Functions / GCP Workflows)
  - Причина: Effort → 0 включает Operational Effort → 0

**Для Production (если масштабируется):**
- Миграция на self-hosted (Kubernetes + локальный quantized LLM)
- Оценка TCO после 3-6 месяцев работы MVP

**Рекомендация:** Начать с cloud/managed. Принцип "Усилия → 0" важнее, чем оптимизация затрат на начальном этапе.

---

## Вопрос 5: Темпоральность и Контекст

### Предлагаемое решение:

**Темпоральный граф (обязательно)**

**Схема:**
```cypher
E = (Olga, works_at, Acme, 
     start=2020, end=2022, 
     source=email_id_123, 
     confidence=0.97,
     extracted_at=2025-11-01)
```

**Обоснование:**
- Без темпоральности граф быстро устаревает
- Невозможно отличить текущие связи от прошлых
- Data provenance (source) критичен для "Right to be Forgotten"

**Усложнение RE:** Да, это усложняет extraction. Но современные LLM справляются с этим через structured output:
```json
{
  "relation": "works_at",
  "subject": "Ольга Розет",
  "object": "ВБШД",
  "temporal": {"start": "2018", "end": "present"},
  "confidence": 0.96
}
```

**Рекомендация:** Реализовать темпоральность с самого начала. Миграция со статического на темпоральный граф позже — крайне затратна.

---

## Вопрос 6: Определение "Контакта"

### Предлагаемое решение:

**Многоуровневая фильтрация:**

**Уровень 1: Source-level filtering (на уровне ingestion)**
- Email: Исключить известные spam-домены
- Email: Включить только адреса из "контактов" (не случайные отправители)
- Web: Только домены с репутацией (исключить UGC-сайты)

**Уровень 2: Relation-level filtering (на уровне IE)**
- Извлекать только "профессиональные" связи
- LLM-prompt: "Извлеки только деловые контакты. Игнорируй упоминания в рекламе, спаме, случайных упоминаниях"

**Уровень 3: Confidence-based filtering (на уровне loading)**
- Загружать в граф только E с confidence > 0.95
- Сущности V без хотя бы одной связи E к Ольге отбрасываются

**Определение "релевантного контакта":**
- Человек или организация, с которыми у Ольги Розет существует/существовала **профессиональная связь**
- Типы связей: сотрудничество, партнёрство, клиентские отношения, коллеги, участники совместных проектов
- **Не включается:** случайные упоминания, фолловеры в соцсетях без взаимодействия, spam

---

## ИТОГОВАЯ КОНФИГУРАЦИЯ

На основе ответов выше, рекомендуемая архитектура MVP:

```yaml
phase: MVP
sources: [public_web, linkedin, media]
llm: api_based (GPT-4o / Claude)
infrastructure: managed_cloud (AWS/GCP)
graph_type: temporal_lpg
precision_threshold: 0.95
entity_resolution_threshold: 0.99
deployment: serverless
estimated_tco: $100-300/месяц
estimated_effort: 0 (после развёртывания)
```

---

## СЛЕДУЮЩИЕ ШАГИ

1. **Согласование с Ольгой:**
   - Подтвердить приоритет Precision > Recall
   - Подтвердить допустимость cloud deployment
   - Подтвердить бюджет (~$100-300/мес для MVP)

2. **Создание TASK_SOW.md:**
   - Формализованное ТЗ для реализации MVP на основе согласованных ответов

3. **Реализация MVP:**
   - Estimated timeline: 2-4 недели разработки
   - Estimated effort: 0 после запуска (self-healing, self-scheduling)

